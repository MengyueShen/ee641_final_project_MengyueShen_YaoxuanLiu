# æ•°æ®é›†å‡†å¤‡æŒ‡å—

## ğŸ“š æ•°æ®é›†å¼•ç”¨

å¦‚æœä½¿ç”¨CUB-200-2011æ•°æ®é›†ï¼Œè¯·åœ¨è®ºæ–‡/æŠ¥å‘Šä¸­å¼•ç”¨ï¼š

```bibtex
@techreport{WahCUB_200_2011,
    Title = {The Caltech-UCSD Birds-200-2011 Dataset},
    Author = {Wah, C. and Branson, S. and Welinder, P. and Perona, P. and Belongie, S.},
    Year = {2011},
    Institution = {California Institute of Technology},
    Number = {CNS-TR-2011-001}
}
```

**é‡è¦æç¤º**ï¼šæ•°æ®é›†ä»…ç”¨äº**éå•†ä¸šç ”ç©¶å’Œæ•™è‚²ç›®çš„**

---

## ğŸ“¥ æ•°æ®é›†ä¸‹è½½

### âš ï¸ é‡è¦ï¼šå¿…é¡»ä½¿ç”¨æµè§ˆå™¨ä¸‹è½½

CUB-200æ•°æ®é›†**è¾ƒå¤§ï¼ˆçº¦1.1GBï¼‰**ï¼Œç›´æ¥ä¸‹è½½é“¾æ¥å¯èƒ½å¤±æ•ˆï¼Œ**å¿…é¡»ä½¿ç”¨æµè§ˆå™¨ä¸‹è½½**ã€‚

### ä¸‹è½½æ­¥éª¤

**æ­¥éª¤1ï¼šè®¿é—®ä¸‹è½½é¡µé¢**

æ‰“å¼€æµè§ˆå™¨ï¼Œè®¿é—®ï¼š
**http://www.vision.caltech.edu/visipedia/CUB-200-2011.html**

**æ­¥éª¤2ï¼šä¸‹è½½æ•°æ®é›†**

1. åœ¨é¡µé¢ä¸Šæ‰¾åˆ° **"Download"** éƒ¨åˆ†
2. ç‚¹å‡»ä¸‹è½½ **"Images and annotations (1.1 GB)"**
3. æ–‡ä»¶åä¸º `CUB_200_2011.tgz`
4. **æ–‡ä»¶å¤§å°åº”è¯¥çº¦1.1GB**ï¼ˆå¦‚æœåªæœ‰å‡ KBï¼Œè¯´æ˜ä¸‹è½½å¤±è´¥ï¼‰

**æ­¥éª¤3ï¼šç§»åŠ¨åˆ°é¡¹ç›®ç›®å½•å¹¶è§£å‹**

```bash
# ç§»åŠ¨åˆ°dataç›®å½•ï¼ˆæ ¹æ®ä½ çš„å®é™…ä¸‹è½½ä½ç½®è°ƒæ•´ï¼‰
mv ~/Downloads/CUB_200_2011.tgz "/Users/smy/Documents/learning/studying/learning/641deeplearning system/641_final_project/data/"

# è¿›å…¥dataç›®å½•
cd "/Users/smy/Documents/learning/studying/learning/641deeplearning system/641_final_project/data"

# éªŒè¯æ–‡ä»¶å¤§å°ï¼ˆåº”è¯¥çº¦1.1GBï¼‰
ls -lh CUB_200_2011.tgz

# è§£å‹
tar -xzf CUB_200_2011.tgz
```

**æ­¥éª¤4ï¼šéªŒè¯è§£å‹ç»“æœ**

```bash
ls CUB_200_2011/
# åº”è¯¥çœ‹åˆ°:
# - images.txt
# - train_test_split.txt
# - images/ (æ–‡ä»¶å¤¹ï¼ŒåŒ…å«å¾ˆå¤šå­æ–‡ä»¶å¤¹)
# - text/ (å¯é€‰ï¼Œæ–‡æœ¬æè¿°æ–‡ä»¶å¤¹)
```

### âš ï¸ å¸¸è§é—®é¢˜

**Q: ä¸‹è½½çš„æ–‡ä»¶åªæœ‰å‡ KBï¼Ÿ**

**A**: ä¸‹è½½å¤±è´¥ï¼Œå¯èƒ½æ˜¯ï¼š
- ç›´æ¥è®¿é—®ä¸‹è½½é“¾æ¥è¿”å›äº†404é¡µé¢
- éœ€è¦ä½¿ç”¨æµè§ˆå™¨ä»ç½‘ç«™é¡µé¢ä¸‹è½½

**è§£å†³**ï¼šå¿…é¡»ä½¿ç”¨æµè§ˆå™¨è®¿é—®ç½‘ç«™é¡µé¢ï¼Œç„¶åç‚¹å‡»ä¸‹è½½é“¾æ¥

**Q: tarè§£å‹å¤±è´¥ï¼Ÿ**

**A**: æ£€æŸ¥ï¼š
- æ–‡ä»¶æ˜¯å¦å®Œæ•´ä¸‹è½½ï¼ˆåº”è¯¥çº¦1.1GBï¼Œä¸æ˜¯å‡ KBï¼‰
- å¦‚æœæ–‡ä»¶å¾ˆå°ï¼Œè¯´æ˜ä¸‹è½½å¤±è´¥ï¼Œéœ€è¦é‡æ–°ä¸‹è½½

---

## ğŸ“Š æ•°æ®é›†åˆ’åˆ†è¯´æ˜ï¼ˆé‡è¦ï¼ï¼‰

### ä¸ºä»€ä¹ˆéœ€è¦åˆ’åˆ†æ•°æ®é›†ï¼Ÿ

åœ¨æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­ï¼Œæ•°æ®é›†é€šå¸¸éœ€è¦åˆ’åˆ†ä¸ºä¸‰ä¸ªéƒ¨åˆ†ï¼š

1. **è®­ç»ƒé›†ï¼ˆTraining Setï¼‰**ï¼šç”¨äºè®­ç»ƒæ¨¡å‹
2. **éªŒè¯é›†ï¼ˆValidation Setï¼‰**ï¼šç”¨äºè°ƒæ•´è¶…å‚æ•°ã€é€‰æ‹©æ¨¡å‹ã€æ—©åœç­‰
3. **æµ‹è¯•é›†ï¼ˆTest Setï¼‰**ï¼šç”¨äºæœ€ç»ˆè¯„ä¼°æ¨¡å‹æ€§èƒ½ï¼ˆ**åªç”¨ä¸€æ¬¡**ï¼‰

### åˆ’åˆ†æ¯”ä¾‹å»ºè®®

å¯¹äºGANè®­ç»ƒï¼Œå¸¸è§çš„åˆ’åˆ†æ¯”ä¾‹ï¼š

- **è®­ç»ƒé›†**ï¼š70-80%ï¼ˆç”¨äºè®­ç»ƒæ¨¡å‹ï¼‰
- **éªŒè¯é›†**ï¼š10-15%ï¼ˆç”¨äºè°ƒæ•´è¶…å‚æ•°ã€è¯„ä¼°è®­ç»ƒè¿›åº¦ï¼‰
- **æµ‹è¯•é›†**ï¼š10-15%ï¼ˆç”¨äºæœ€ç»ˆè¯„ä¼°ï¼Œ**è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä½¿ç”¨**ï¼‰

**ç¤ºä¾‹**ï¼ˆCUB-200æ•°æ®é›†ï¼Œå…±11,788å¼ å›¾åƒï¼‰ï¼š
- è®­ç»ƒé›†ï¼š~8,000å¼ ï¼ˆçº¦68%ï¼‰
- éªŒè¯é›†ï¼š~1,800å¼ ï¼ˆçº¦15%ï¼‰
- æµ‹è¯•é›†ï¼š~1,988å¼ ï¼ˆçº¦17%ï¼‰

---

## ğŸ“ CUB-200æ•°æ®é›†åˆ’åˆ†

### æ•°æ®é›†è‡ªå¸¦åˆ’åˆ†

CUB-200æ•°æ®é›†**å·²ç»æä¾›äº†åˆ’åˆ†**ï¼š

- `train_test_split.txt`ï¼šåŒ…å«æ¯å¼ å›¾åƒçš„åˆ’åˆ†ä¿¡æ¯
  - `1` = è®­ç»ƒé›†
  - `0` = æµ‹è¯•é›†

### å¦‚ä½•ä½¿ç”¨

```python
from src.utils.datasets import CUB200Dataset

# è®­ç»ƒé›†
train_dataset = CUB200Dataset(
    root_dir='./data/CUB_200_2011',
    split='train',  # ä½¿ç”¨train_test_split.txtä¸­çš„è®­ç»ƒé›†
    max_text_length=18
)

# æµ‹è¯•é›†
test_dataset = CUB200Dataset(
    root_dir='./data/CUB_200_2011',
    split='test',  # ä½¿ç”¨train_test_split.txtä¸­çš„æµ‹è¯•é›†
    max_text_length=18
)
```

### å¦‚ä½•åˆ›å»ºéªŒè¯é›†ï¼Ÿ

**æ–¹æ³•1ï¼šä»è®­ç»ƒé›†ä¸­å†åˆ’åˆ†ï¼ˆæ¨èï¼‰**

```python
from torch.utils.data import random_split

# å…ˆåŠ è½½è®­ç»ƒé›†
full_train_dataset = CUB200Dataset('./data/CUB_200_2011', split='train')

# ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†å‡ºéªŒè¯é›†ï¼ˆä¾‹å¦‚20%ï¼‰
train_size = int(0.8 * len(full_train_dataset))
val_size = len(full_train_dataset) - train_size
train_dataset, val_dataset = random_split(full_train_dataset, [train_size, val_size])
```

**æ–¹æ³•2ï¼šæ‰‹åŠ¨ä¿®æ”¹train_test_split.txt**

ä¸æ¨èï¼Œå› ä¸ºä¼šæ”¹å˜åŸå§‹æ•°æ®é›†åˆ’åˆ†ã€‚

---

## ğŸ“ COCOæ•°æ®é›†åˆ’åˆ†

### COCOæ•°æ®é›†è‡ªå¸¦åˆ’åˆ†

COCOæ•°æ®é›†å·²ç»æä¾›äº†åˆ’åˆ†ï¼š

- `train2017/`ï¼šè®­ç»ƒé›†å›¾åƒ
- `val2017/`ï¼šéªŒè¯é›†å›¾åƒï¼ˆ**æ³¨æ„ï¼šCOCOçš„valå®é™…ä¸Šæ˜¯éªŒè¯é›†**ï¼‰
- `annotations/captions_train2017.json`ï¼šè®­ç»ƒé›†æ ‡æ³¨
- `annotations/captions_val2017.json`ï¼šéªŒè¯é›†æ ‡æ³¨

### å¦‚ä½•ä½¿ç”¨

```python
from src.utils.datasets import COCODataset

# è®­ç»ƒé›†
train_dataset = COCODataset(
    root_dir='./data/COCO',
    split='train',
    max_text_length=18
)

# éªŒè¯é›†ï¼ˆCOCOçš„valå°±æ˜¯éªŒè¯é›†ï¼‰
val_dataset = COCODataset(
    root_dir='./data/COCO',
    split='val',
    max_text_length=18
)
```

### å¦‚ä½•åˆ›å»ºæµ‹è¯•é›†ï¼Ÿ

**æ–¹æ³•ï¼šä»éªŒè¯é›†ä¸­åˆ’åˆ†**

```python
from torch.utils.data import random_split

# å…ˆåŠ è½½éªŒè¯é›†
full_val_dataset = COCODataset('./data/COCO', split='val')

# ä»éªŒè¯é›†ä¸­åˆ’åˆ†å‡ºæµ‹è¯•é›†ï¼ˆä¾‹å¦‚50%ï¼‰
val_size = int(0.5 * len(full_val_dataset))
test_size = len(full_val_dataset) - val_size
val_dataset, test_dataset = random_split(full_val_dataset, [val_size, test_size])
```

---

## ğŸ¯ åœ¨è®­ç»ƒä¸­çš„ä½¿ç”¨

### è®­ç»ƒé›†ï¼ˆTraining Setï¼‰

**ç”¨é€”**ï¼š
- è®­ç»ƒGANæ¨¡å‹ï¼ˆGeneratorå’ŒDiscriminatorï¼‰
- æ›´æ–°æ¨¡å‹å‚æ•°

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)

for batch in train_loader:
    # è®­ç»ƒæ¨¡å‹
    train_step(batch)
```

---

### éªŒè¯é›†ï¼ˆValidation Setï¼‰

**ç”¨é€”**ï¼š
- **è¯„ä¼°è®­ç»ƒè¿›åº¦**ï¼šæ¯Nä¸ªepochåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
- **è°ƒæ•´è¶…å‚æ•°**ï¼šé€‰æ‹©æœ€ä½³å­¦ä¹ ç‡ã€batch sizeç­‰
- **æ—©åœï¼ˆEarly Stoppingï¼‰**ï¼šå¦‚æœéªŒè¯é›†æ€§èƒ½ä¸å†æå‡ï¼Œåœæ­¢è®­ç»ƒ
- **é€‰æ‹©æœ€ä½³æ¨¡å‹**ï¼šä¿å­˜éªŒè¯é›†ä¸Šæ€§èƒ½æœ€å¥½çš„æ¨¡å‹

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

# æ¯5ä¸ªepochè¯„ä¼°ä¸€æ¬¡
if epoch % 5 == 0:
    val_metrics = evaluate(val_loader)
    print(f"Validation FID: {val_metrics['fid']}")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    if val_metrics['fid'] < best_fid:
        save_checkpoint(model, 'best_model.pth')
```

---

### æµ‹è¯•é›†ï¼ˆTest Setï¼‰

**ç”¨é€”**ï¼š
- **æœ€ç»ˆè¯„ä¼°**ï¼šè®­ç»ƒå®Œæˆåï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹
- **æŠ¥å‘Šç»“æœ**ï¼šè®ºæ–‡/æŠ¥å‘Šä¸­æŠ¥å‘Šçš„æ€§èƒ½æŒ‡æ ‡
- **åªä½¿ç”¨ä¸€æ¬¡**ï¼šè®­ç»ƒè¿‡ç¨‹ä¸­**ä¸åº”è¯¥**ä½¿ç”¨æµ‹è¯•é›†

**ä½¿ç”¨æ–¹å¼**ï¼š
```python
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)

# è®­ç»ƒå®Œæˆåï¼Œæœ€ç»ˆè¯„ä¼°
final_metrics = evaluate(test_loader)
print(f"Final Test FID: {final_metrics['fid']}")
print(f"Final Test IS: {final_metrics['is']}")
print(f"Final Test CLIP: {final_metrics['clip_score']}")
```

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### 1. ä¸è¦ç”¨æµ‹è¯•é›†è®­ç»ƒï¼

- âŒ **é”™è¯¯**ï¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨æµ‹è¯•é›†è°ƒæ•´æ¨¡å‹
- âœ… **æ­£ç¡®**ï¼šæµ‹è¯•é›†åªåœ¨æœ€åè¯„ä¼°æ—¶ä½¿ç”¨ä¸€æ¬¡

### 2. ä¿æŒåˆ’åˆ†ä¸€è‡´æ€§

- ä½¿ç”¨å›ºå®šçš„éšæœºç§å­ï¼ˆseedï¼‰ç¡®ä¿åˆ’åˆ†å¯å¤ç°
- ä¸è¦åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ”¹å˜åˆ’åˆ†

### 3. éªŒè¯é›†çš„ä½œç”¨

- éªŒè¯é›†ç”¨äº"æ¨¡æ‹Ÿ"æµ‹è¯•é›†
- åœ¨éªŒè¯é›†ä¸Šè¡¨ç°å¥½ï¼Œé€šå¸¸æµ‹è¯•é›†ä¸Šä¹Ÿä¼šè¡¨ç°å¥½
- ä½†å¦‚æœéªŒè¯é›†å’Œæµ‹è¯•é›†åˆ†å¸ƒä¸åŒï¼Œå¯èƒ½ä¸æˆç«‹

---

## ğŸ“ æ•°æ®é›†åˆ’åˆ†ç¤ºä¾‹ä»£ç 

### å®Œæ•´ç¤ºä¾‹ï¼ˆCUB-200ï¼‰

```python
import torch
from torch.utils.data import DataLoader, random_split
from src.utils.datasets import CUB200Dataset, collate_fn

# 1. åŠ è½½è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼ˆæ•°æ®é›†è‡ªå¸¦åˆ’åˆ†ï¼‰
train_dataset = CUB200Dataset('./data/CUB_200_2011', split='train')
test_dataset = CUB200Dataset('./data/CUB_200_2011', split='test')

# 2. ä»è®­ç»ƒé›†ä¸­åˆ’åˆ†å‡ºéªŒè¯é›†
train_size = int(0.8 * len(train_dataset))
val_size = len(train_dataset) - train_size
train_dataset, val_dataset = random_split(
    train_dataset, 
    [train_size, val_size],
    generator=torch.Generator().manual_seed(42)  # å›ºå®šéšæœºç§å­
)

# 3. åˆ›å»ºDataLoader
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)
val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)

print(f"è®­ç»ƒé›†: {len(train_dataset)} å¼ ")
print(f"éªŒè¯é›†: {len(val_dataset)} å¼ ")
print(f"æµ‹è¯•é›†: {len(test_dataset)} å¼ ")
```

---

## ğŸ¯ ä¸­æœŸæŠ¥å‘Šä¸­çš„æ•°æ®é›†è¯´æ˜

åœ¨ä¸­æœŸæŠ¥å‘Šä¸­ï¼Œä½ åº”è¯¥è¯´æ˜ï¼š

1. **ä½¿ç”¨çš„æ•°æ®é›†**ï¼šCUB-200æˆ–COCO
2. **æ•°æ®é›†å¤§å°**ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†çš„æ•°é‡
3. **åˆ’åˆ†æ–¹å¼**ï¼šå¦‚ä½•åˆ’åˆ†çš„ï¼ˆæ•°æ®é›†è‡ªå¸¦åˆ’åˆ† or æ‰‹åŠ¨åˆ’åˆ†ï¼‰
4. **åˆ’åˆ†æ¯”ä¾‹**ï¼šè®­ç»ƒ/éªŒè¯/æµ‹è¯•çš„æ¯”ä¾‹

**ç¤ºä¾‹æ–‡å­—**ï¼š

"æˆ‘ä»¬ä½¿ç”¨CUB-200-2011æ•°æ®é›†è¿›è¡Œå®éªŒï¼Œè¯¥æ•°æ®é›†åŒ…å«11,788å¼ é¸Ÿç±»å›¾åƒï¼Œæ¯å¼ å›¾åƒæœ‰10ä¸ªæ–‡æœ¬æè¿°ã€‚æ•°æ®é›†æä¾›äº†è®­ç»ƒé›†å’Œæµ‹è¯•é›†çš„å®˜æ–¹åˆ’åˆ†ï¼ˆtrain_test_split.txtï¼‰ã€‚æˆ‘ä»¬ä»è®­ç»ƒé›†ä¸­è¿›ä¸€æ­¥åˆ’åˆ†å‡º20%ä½œä¸ºéªŒè¯é›†ï¼Œæœ€ç»ˆå¾—åˆ°ï¼š
- è®­ç»ƒé›†ï¼š8,000å¼ å›¾åƒï¼ˆç”¨äºæ¨¡å‹è®­ç»ƒï¼‰
- éªŒè¯é›†ï¼š1,800å¼ å›¾åƒï¼ˆç”¨äºè¶…å‚æ•°è°ƒæ•´å’Œè®­ç»ƒè¿›åº¦è¯„ä¼°ï¼‰
- æµ‹è¯•é›†ï¼š1,988å¼ å›¾åƒï¼ˆç”¨äºæœ€ç»ˆæ€§èƒ½è¯„ä¼°ï¼Œè®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä½¿ç”¨ï¼‰"

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ  
**å…³é”®ç‚¹**: è®­ç»ƒé›†ç”¨äºè®­ç»ƒï¼ŒéªŒè¯é›†ç”¨äºè°ƒæ•´å’Œè¯„ä¼°ï¼Œæµ‹è¯•é›†åªåœ¨æœ€åç”¨ä¸€æ¬¡ï¼

