# å¦‚ä½•è¿è¡Œä»£ç  - å¿«é€Ÿæ“ä½œæŒ‡å—

## ğŸ“‹ é¡¹ç›®æ˜¯ä»€ä¹ˆï¼Ÿ

ç ”ç©¶**ä¸‰ç§Scheduleæœºåˆ¶**åœ¨æ–‡æœ¬åˆ°å›¾åƒGANè®­ç»ƒä¸­çš„æ•ˆæœï¼š
1. **Fixed Annealing**ï¼ˆå›ºå®šé€€ç«ï¼‰- ä½¿ç”¨æ•°å­¦å…¬å¼
2. **Learnable Monotone**ï¼ˆå¯å­¦ä¹ å•è°ƒï¼‰- K-bin softmaxå­¦ä¹ 
3. **Adaptive Annealing**ï¼ˆè‡ªé€‚åº”é€€ç«ï¼‰- å›ºå®šé€€ç«+å°æ§åˆ¶å™¨

---

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼ˆ3æ­¥ï¼‰

### æ­¥éª¤1ï¼šå®‰è£…ä¾èµ–

```bash
pip install -r requirements.txt
```

### æ­¥éª¤2ï¼šå‡†å¤‡æ•°æ®é›†

**é€‰é¡¹Aï¼šä½¿ç”¨CUB-200ï¼ˆæ¨èï¼Œå¿«é€ŸéªŒè¯ï¼‰**
```bash
cd data/
# ä¸‹è½½åœ°å€: http://www.vision.caltech.edu/visipedia/CUB-200-2011.html
wget http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz
tar -xzf CUB_200_2011.tgz
```

**ç›®å½•ç»“æ„**:
```
CUB_200_2011/
â”œâ”€â”€ images.txt              # å›¾åƒåˆ—è¡¨
â”œâ”€â”€ train_test_split.txt    # è®­ç»ƒ/æµ‹è¯•åˆ’åˆ†
â”œâ”€â”€ images/                 # å›¾åƒæ–‡ä»¶å¤¹
â””â”€â”€ text/                   # æ–‡æœ¬æè¿°æ–‡ä»¶å¤¹ï¼ˆå¯é€‰ï¼‰
```

**é€‰é¡¹Bï¼šä½¿ç”¨MS-COCOï¼ˆproposalæŒ‡å®šï¼Œè¾ƒå¤§ï¼‰**
```bash
cd data/
# ä¸‹è½½åœ°å€: https://cocodataset.org/
# éœ€è¦ä¸‹è½½: train2017.zip, val2017.zip, annotations_trainval2017.zip
# è§£å‹åç»“æ„:
# COCO/
#   â”œâ”€â”€ train2017/          # è®­ç»ƒå›¾åƒ
#   â”œâ”€â”€ val2017/            # éªŒè¯å›¾åƒ
#   â””â”€â”€ annotations/        # æ ‡æ³¨æ–‡ä»¶
```

**éªŒè¯æ•°æ®é›†**:
```python
from src.utils.datasets import CUB200Dataset
dataset = CUB200Dataset('./data/CUB_200_2011', split='train')
print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"è¯æ±‡è¡¨å¤§å°: {dataset.vocab_size}")
```

### æ­¥éª¤3ï¼šè¿è¡Œè®­ç»ƒ

```bash
# ä½¿ç”¨Fixed Annealing
python src/experiments/train_text2image.py

# æˆ–ä¿®æ”¹ä»£ç ä¸­çš„scheduler_typeæ¥é€‰æ‹©ä¸åŒçš„è°ƒåº¦å™¨
```

---

## ğŸ“ ä»£ç ç»“æ„ï¼ˆç®€å•ç†è§£ï¼‰

```
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ text_to_image_gan.py  # æ–‡æœ¬åˆ°å›¾åƒGANï¼ˆä¸»è¦ç”¨è¿™ä¸ªï¼‰
â”‚   â””â”€â”€ gan.py                 # åŸºç¡€GANï¼ˆå¯¹æ¯”ç”¨ï¼‰
â”‚
â”œâ”€â”€ schedulers/                # Scheduleæœºåˆ¶ï¼ˆæ ¸å¿ƒï¼‰
â”‚   â”œâ”€â”€ annealed.py           # Fixed Annealing âœ…
â”‚   â”œâ”€â”€ learnable_monotone.py # Learnable Monotone âœ…
â”‚   â”œâ”€â”€ adaptive_annealed.py  # Adaptive Annealing âœ…
â”‚   â””â”€â”€ learnable.py          # MLPå­¦ä¹ ï¼ˆå¯¹æ¯”ç”¨ï¼‰
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ datasets.py           # æ•°æ®é›†åŠ è½½
â”‚
â””â”€â”€ experiments/
    â””â”€â”€ train_text2image.py   # è®­ç»ƒè„šæœ¬
```

---

## ğŸ’» ä»£ç ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1ï¼šä½¿ç”¨Fixed Annealing

```python
from src.models.text_to_image_gan import TextToImageGAN
from src.schedulers.annealed import AnnealedScheduler
from src.utils.datasets import CUB200Dataset, collate_fn
from torch.utils.data import DataLoader

# 1. åŠ è½½æ•°æ®
dataset = CUB200Dataset('./data/CUB_200_2011', split='train')
dataloader = DataLoader(dataset, batch_size=32, collate_fn=collate_fn)

# 2. åˆ›å»ºæ¨¡å‹
gan = TextToImageGAN(vocab_size=dataset.vocab_size, device='cuda')

# 3. åˆ›å»ºè°ƒåº¦å™¨
config = AnnealedScheduler.create_default_config()
scheduler = AnnealedScheduler(config)

# 4. è®­ç»ƒå¾ªç¯
for epoch in range(100):
    scheduler.update(epoch, 100)  # æ›´æ–°å‚æ•°
    params = scheduler.get_parameters()
    
    for batch in dataloader:
        # ä½¿ç”¨params['noise_var']ç­‰å‚æ•°è®­ç»ƒGAN
        # ... è®­ç»ƒä»£ç 
```

### ç¤ºä¾‹2ï¼šä½¿ç”¨Learnable Monotone

```python
from src.schedulers.learnable_monotone import LearnableMonotoneScheduler

# åˆ›å»ºå¯å­¦ä¹ å•è°ƒè°ƒåº¦å™¨
config = LearnableMonotoneScheduler.create_default_config()
scheduler = LearnableMonotoneScheduler(config, k_bins=10, device='cuda')

# è®­ç»ƒæ—¶ï¼Œscheduleå‚æ•°ä¼šé€šè¿‡GANæŸå¤±è‡ªåŠ¨å­¦ä¹ 
# éœ€è¦å°†scheduleå‚æ•°åŠ å…¥ä¼˜åŒ–å™¨ï¼ˆè§ä¸‹æ–¹"è®­ç»ƒé€»è¾‘"ï¼‰
```

### ç¤ºä¾‹3ï¼šä½¿ç”¨Adaptive Annealing

```python
from src.schedulers.adaptive_annealed import AdaptiveAnnealedScheduler

config = AdaptiveAnnealedScheduler.create_default_config()
state_features = ['loss_g', 'loss_d', 'grad_norm_g', 'grad_norm_d', 'epoch_progress']
scheduler = AdaptiveAnnealedScheduler(config, state_features, device='cuda')

# æ›´æ–°æ—¶éœ€è¦æä¾›è®­ç»ƒçŠ¶æ€
scheduler.update(epoch, 100,
                generator=gan.generator,
                discriminator=gan.discriminator,
                losses={'g': loss_g, 'd': loss_d})
```

---

## âš ï¸ å½“å‰ä»£ç çŠ¶æ€

### âœ… å¯ä»¥è¿è¡Œçš„

- âœ… Fixed Annealingï¼šå®Œå…¨å¯ç”¨
- âœ… æ–‡æœ¬åˆ°å›¾åƒGANæ¨¡å‹ï¼šå¯ä»¥è®­ç»ƒ
- âœ… æ•°æ®é›†åŠ è½½ï¼šCUB-200å’ŒCOCOéƒ½æ”¯æŒ

### âš ï¸ éœ€è¦å®Œå–„çš„

- âš ï¸ LearnableMonotoneï¼šä»£ç å®Œæˆï¼Œä½†**è®­ç»ƒé€»è¾‘æœªå®ç°**
  - éœ€è¦å°†scheduleå‚æ•°åŠ å…¥ä¼˜åŒ–å™¨
  - é€šè¿‡GANæŸå¤±åå‘ä¼ æ’­æ›´æ–°binæƒé‡

- âš ï¸ AdaptiveAnnealedï¼šä»£ç å®Œæˆï¼Œä½†**è®­ç»ƒé€»è¾‘æœªå®ç°**
  - éœ€è¦å®ç°åŒå±‚ä¼˜åŒ–
  - è®­ç»ƒå°æ§åˆ¶å™¨

- âš ï¸ è®­ç»ƒè„šæœ¬ï¼šæœ‰åŸºç¡€ç‰ˆæœ¬ï¼Œä½†**ä¸å®Œæ•´**
  - ç¼ºå°‘æ¨¡å‹ä¿å­˜
  - ç¼ºå°‘è¯„ä¼°æŒ‡æ ‡
  - ç¼ºå°‘å¯è§†åŒ–

---

## ğŸ”§ å¦‚ä½•è®©ä»£ç è·‘é€šï¼ˆæœ€å°ç‰ˆæœ¬ï¼‰

### æ–¹æ¡ˆ1ï¼šåªç”¨Fixed Annealingï¼ˆæœ€ç®€å•ï¼‰

```python
# ä¿®æ”¹ train_text2image.py
scheduler_type = 'annealed'  # åªç”¨å›ºå®šé€€ç«

# è¿è¡Œ
python src/experiments/train_text2image.py
```

**éœ€è¦åšçš„**ï¼š
1. ä¸‹è½½æ•°æ®é›†
2. ç¡®ä¿ä»£ç æ²¡æœ‰è¯­æ³•é”™è¯¯
3. è¿è¡Œå³å¯

### æ–¹æ¡ˆ2ï¼šæµ‹è¯•LearnableMonotoneï¼ˆéœ€è¦å®Œå–„è®­ç»ƒé€»è¾‘ï¼‰

**éœ€è¦æ·»åŠ çš„ä»£ç **ï¼š
```python
# åœ¨train_text2image.pyä¸­ï¼Œå°†scheduleå‚æ•°åŠ å…¥ä¼˜åŒ–å™¨
optimizer_g = optim.Adam(
    list(gan.generator.parameters()) + 
    list(gan.text_encoder.parameters()) +
    list(scheduler.schedules.parameters()),  # åŠ å…¥scheduleå‚æ•°
    lr=0.0002
)

# è®­ç»ƒæ—¶ï¼Œscheduleå‚æ•°ä¼šé€šè¿‡loss_g.backward()è‡ªåŠ¨æ›´æ–°
```

### æ–¹æ¡ˆ3ï¼šæµ‹è¯•AdaptiveAnnealedï¼ˆéœ€è¦åŒå±‚ä¼˜åŒ–ï¼‰

**éœ€è¦æ·»åŠ çš„ä»£ç **ï¼š
```python
# åˆ›å»ºæ§åˆ¶å™¨ä¼˜åŒ–å™¨
optimizer_controller = optim.Adam(
    scheduler.controllers.parameters(),
    lr=0.001  # è¾ƒå°çš„å­¦ä¹ ç‡
)

# æ¯Kä¸ªepochæ›´æ–°æ§åˆ¶å™¨
if epoch % 5 == 0:
    # åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
    fid_score = evaluate_fid(...)
    
    # æ›´æ–°æ§åˆ¶å™¨
    optimizer_controller.zero_grad()
    fid_score.backward()  # éœ€è¦è®¾è®¡å¦‚ä½•åå‘ä¼ æ’­
    optimizer_controller.step()
```

---

## ğŸ“ æœ€å°å¯è¿è¡Œç‰ˆæœ¬ï¼ˆFixed Annealingï¼‰

åˆ›å»ºä¸€ä¸ªæœ€ç®€å•çš„å¯è¿è¡Œè„šæœ¬ï¼š

```python
# simple_train.py
import torch
from src.models.text_to_image_gan import TextToImageGAN
from src.schedulers.annealed import AnnealedScheduler
from src.utils.datasets import CUB200Dataset, collate_fn
from torch.utils.data import DataLoader

# 1. æ•°æ®
dataset = CUB200Dataset('./data/CUB_200_2011', split='train')
dataloader = DataLoader(dataset, batch_size=4, collate_fn=collate_fn)

# 2. æ¨¡å‹
gan = TextToImageGAN(vocab_size=dataset.vocab_size, device='cuda')

# 3. è°ƒåº¦å™¨
scheduler = AnnealedScheduler(AnnealedScheduler.create_default_config())

# 4. è®­ç»ƒï¼ˆç®€åŒ–ç‰ˆï¼‰
for epoch in range(10):
    scheduler.update(epoch, 10)
    params = scheduler.get_parameters()
    print(f"Epoch {epoch}: noise_var={params['noise_var']:.4f}")
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ å®é™…çš„è®­ç»ƒä»£ç 
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æ•°æ®é›†æ‰¾ä¸åˆ°ï¼Ÿ

**A**: 
- æ£€æŸ¥è·¯å¾„ï¼š`./data/CUB_200_2011/` æ˜¯å¦å­˜åœ¨
- æ£€æŸ¥æ–‡ä»¶ï¼šç¡®ä¿æœ‰`images.txt`å’Œ`train_test_split.txt`

### Q2: è¯æ±‡è¡¨é”™è¯¯ï¼Ÿ

**A**: 
- ç¡®ä¿æ•°æ®é›†å·²æ­£ç¡®åŠ è½½
- æ£€æŸ¥`dataset.vocab_size`æ˜¯å¦æ­£ç¡®

### Q3: CUDA out of memoryï¼Ÿ

**A**: 
- å‡å°batch_sizeï¼ˆæ”¹ä¸º4æˆ–8ï¼‰
- ä½¿ç”¨CPUï¼š`device='cpu'`

### Q4: å¦‚ä½•åˆ‡æ¢ä¸åŒçš„è°ƒåº¦å™¨ï¼Ÿ

**A**: ä¿®æ”¹`train_text2image.py`ä¸­çš„`scheduler_type`ï¼š
```python
scheduler_type = 'annealed'  # æˆ– 'learnable_monotone' æˆ– 'adaptive'
```

---

## ğŸ“Š ä¸‰ç§Scheduleæœºåˆ¶å¯¹æ¯”

| æœºåˆ¶ | æ–‡ä»¶ | æ˜¯å¦éœ€è¦è®­ç»ƒ | éš¾åº¦ |
|------|------|------------|------|
| Fixed Annealing | `annealed.py` | âŒ ä¸éœ€è¦ | â­ ç®€å• |
| Learnable Monotone | `learnable_monotone.py` | âœ… éœ€è¦ | â­â­â­ ä¸­ç­‰ |
| Adaptive Annealing | `adaptive_annealed.py` | âœ… éœ€è¦ | â­â­â­â­ è¾ƒéš¾ |

**å»ºè®®**ï¼šå…ˆä»Fixed Annealingå¼€å§‹ï¼Œç¡®ä¿ä»£ç èƒ½è·‘é€šï¼Œå†é€æ­¥æ·»åŠ å…¶ä»–æœºåˆ¶ã€‚

---

## ğŸ¯ ä¸‹ä¸€æ­¥æ“ä½œï¼ˆæŒ‰é¡ºåºï¼‰

1. **ä¸‹è½½æ•°æ®é›†**ï¼ˆå¿…é¡»ï¼‰
2. **æµ‹è¯•Fixed Annealing**ï¼ˆç¡®ä¿åŸºç¡€åŠŸèƒ½æ­£å¸¸ï¼‰
3. **å®Œå–„è®­ç»ƒè„šæœ¬**ï¼ˆæ·»åŠ ä¿å­˜ã€æ—¥å¿—ç­‰ï¼‰
4. **å®ç°è¯„ä¼°æŒ‡æ ‡**ï¼ˆFIDç­‰ï¼‰
5. **å®ç°LearnableMonotoneè®­ç»ƒé€»è¾‘**
6. **å®ç°AdaptiveAnnealedè®­ç»ƒé€»è¾‘**

---

---

## ğŸš€ è¿è¡ŒFixed Annealingå®Œæ•´å®éªŒ

### æ­¥éª¤1ï¼šä¸‹è½½æ•°æ®é›†

è§ [DATASET_GUIDE.md](DATASET_GUIDE.md) - æ•°æ®é›†å‡†å¤‡æŒ‡å—

### æ­¥éª¤2ï¼šè¿è¡Œå®Œæ•´è®­ç»ƒ

```bash
python src/experiments/train_full_pipeline.py
```

**è¿™ä¸ªè„šæœ¬ä¼š**ï¼š
- âœ… è‡ªåŠ¨åŠ è½½æ•°æ®é›†å¹¶åˆ’åˆ†ï¼ˆè®­ç»ƒé›†80%ï¼ŒéªŒè¯é›†20%ï¼Œæµ‹è¯•é›†ä½¿ç”¨å®˜æ–¹åˆ’åˆ†ï¼‰
- âœ… è®­ç»ƒ50ä¸ªepochï¼ˆä½¿ç”¨Fixed Annealingï¼‰
- âœ… æ¯5ä¸ªepochåœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
- âœ… è®°å½•æ‰€æœ‰æŸå¤±ã€æ¢¯åº¦ã€scheduleå‚æ•°
- âœ… ç”Ÿæˆæ‰€æœ‰å¯è§†åŒ–å›¾åƒ
- âœ… ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹

### æ­¥éª¤3ï¼šæŸ¥çœ‹ç»“æœ

**ç”Ÿæˆçš„æ–‡ä»¶**ï¼š
- `results/figures/metrics_vs_steps_fixed_annealing.png` - FID/IS/CLIP vs Steps â­
- `results/figures/loss_curves_fixed_annealing.png` - Lossæ›²çº¿ â­
- `results/figures/schedule_params_fixed_annealing.png` - Scheduleå‚æ•°æ›²çº¿ â­â­â­æ ¸å¿ƒ
- `results/figures/grad_norms_fixed_annealing.png` - æ¢¯åº¦èŒƒæ•°
- `results/checkpoints/best_model.pth` - æœ€ä½³æ¨¡å‹
- `results/training_data.json` - è®­ç»ƒæ•°æ®
- `results/final_results.json` - æœ€ç»ˆç»“æœ

**è¿™äº›å›¾åƒå¯ç›´æ¥ç”¨äºä¸­æœŸæŠ¥å‘Šçš„Results to Dateéƒ¨åˆ†**

---

**æœ€åæ›´æ–°**: 2024å¹´12æœˆ  
**æ ¸å¿ƒæ–‡ä»¶**: `src/experiments/train_full_pipeline.py` - è¿è¡Œè¿™ä¸ªå³å¯ï¼

